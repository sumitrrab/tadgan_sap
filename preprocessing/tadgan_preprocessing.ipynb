{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28717887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with >90% missing: ['bvorg', 'stblg', 'stjah', 'bktxt', 'bstat', 'grpid', 'xsnet', 'ausbk', 'stgrd', 'xreversal', 'augcp', 'augbl', 'umskz', 'umsks', 'txbhw', 'txbfw', 'hwbas', 'fwbas', 'mwart', 'sgtxt', 'vbund', 'kostl', 'vbeln', 'vbel2', 'posn2', 'xhres', 'xskrl', 'xauto', 'xzahl', 'zbd3t', 'zbd2p', 'sknto', 'wskto', 'zlspr', 'nebtr', 'mwsk1', 'dmbt1', 'dmbt2', 'rebzg', 'rebzj', 'rebzz', 'rebzt', 'landl', 'klibt', 'zekkn', 'bualt', 'rdiff', 'rdif2', 'txjcd', 'sknt2', 'hwmet', 'xragl', 'stbuk', 'txbh2', 'pprct', 'xnegp', 'kkber', 'agzei', 'auggj', 'segment', 'kstar']\n",
      "Columns with 20-90% missing: ['xblnr', 'kursf', 'xrueb', 'kurs2', 'xmwst', 'doccat', 'buzid', 'gsber', 'mwskz', 'txgrp', 'valut', 'zuonr', 'xumsw', 'xkres', 'xopvw', 'xncop', 'saknr', 'kunnr', 'lifnr', 'gvtyp', 'zfbdt', 'zterm', 'zbd1t', 'zbd2t', 'zbd1p', 'skfbt', 'matnr', 'werks', 'menge', 'meins', 'erfmg', 'erfme', 'bpmng', 'bprme', 'ebeln', 'ebelp', 'vprsv', 'bwkey', 'bustw', 'prctr', 'xref3', 'kidno', 'squan']\n",
      "Columns with <=20% missing: ['mandt', 'bukrs', 'belnr', 'gjahr', 'blart', 'monat', 'cputm', 'wwert', 'usnam', 'tcode', 'waers', 'glvor', 'awtyp', 'awkey', 'hwaer', 'hwae2', 'basw2', 'umrd2', 'curt2', 'kuty2', 'kurst', 'operation_flag_x', 'is_deleted_x', 'recordstamp_x', 'buzei', 'bschl', 'koart', 'shkzg', 'dmbtr', 'wrbtr', 'pswbt', 'pswsl', 'ktosl', 'vorgn', 'kokrs', 'hkont', 'xbilk', 'dmbe2', 'operation_flag_y', 'is_deleted_y', 'recordstamp_y', 'year', 'month', 'day', 'day_of_week', 'quarter', 'month_sin', 'month_cos', 'day_sin', 'day_cos']\n",
      "Identified binary columns: ['stgrd', 'is_deleted_x', 'rebzz', 'zekkn', 'is_deleted_y']\n",
      "High-cardinality categorical columns: ['cputm', 'wwert', 'usnam', 'xblnr', 'awkey', 'recordstamp_x', 'augcp', 'valut', 'zuonr', 'kunnr', 'lifnr', 'zfbdt', 'matnr', 'prctr', 'pprct', 'recordstamp_y']\n",
      "Low-cardinality categorical columns: ['bukrs', 'blart', 'tcode', 'bvorg', 'bktxt', 'waers', 'bstat', 'xrueb', 'glvor', 'grpid', 'awtyp', 'hwaer', 'hwae2', 'xmwst', 'kuty2', 'xsnet', 'ausbk', 'doccat', 'kurst', 'operation_flag_x', 'is_deleted_x', 'buzid', 'koart', 'umskz', 'umsks', 'shkzg', 'gsber', 'mwskz', 'pswsl', 'mwart', 'ktosl', 'sgtxt', 'vbund', 'vorgn', 'kokrs', 'kostl', 'xumsw', 'xhres', 'xkres', 'xopvw', 'xskrl', 'xauto', 'xncop', 'xzahl', 'xbilk', 'gvtyp', 'zlspr', 'mwsk1', 'rebzt', 'landl', 'meins', 'erfme', 'bprme', 'vprsv', 'bustw', 'txjcd', 'hwmet', 'xragl', 'stbuk', 'xnegp', 'kkber', 'segment', 'squan', 'operation_flag_y', 'is_deleted_y']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/3741710448.py:107: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete! Processed dataset saved as '/Users/User2/Desktop/Research Seminar/processed_timegan_data_full.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1. Load the Dataset\n",
    "# ==========================\n",
    "file_path = \"/Users/User2/Desktop/Research Seminar/approach_1_df.csv\"\n",
    "# Use low_memory=False to avoid dtype warnings\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# ==========================\n",
    "# 2. Date Conversion and Feature Extraction\n",
    "# ==========================\n",
    "# Define date columns; note: any invalid date (e.g., wrong format) becomes NaT (Not a Time)\n",
    "date_columns = [\"bldat\", \"budat\", \"cpudt\", \"aedat\", \"augdt\"]\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')  # 'coerce' makes invalid dates into NaT\n",
    "\n",
    "# Sort by posting date (budat) to maintain time order\n",
    "df = df.sort_values(by=\"budat\")\n",
    "\n",
    "# Extract date features from 'budat'\n",
    "df[\"year\"] = df[\"budat\"].dt.year\n",
    "df[\"month\"] = df[\"budat\"].dt.month\n",
    "df[\"day\"] = df[\"budat\"].dt.day\n",
    "df[\"day_of_week\"] = df[\"budat\"].dt.weekday  # Monday=0, Sunday=6\n",
    "df[\"quarter\"] = df[\"budat\"].dt.quarter\n",
    "\n",
    "# Create cyclical (sin/cos) encoding for month and day-of-week to capture periodicity\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "# Option: If you no longer need the original date columns, drop them.\n",
    "df = df.drop(columns=date_columns)\n",
    "\n",
    "# ==========================\n",
    "# 3. Missing Value Analysis and Imputation\n",
    "# ==========================\n",
    "# Calculate percentage of missing values per column\n",
    "missing_perc = df.isna().mean() * 100\n",
    "\n",
    "# Identify columns based on missing value thresholds\n",
    "high_missing_cols = missing_perc[missing_perc > 90].index.tolist()\n",
    "moderate_missing_cols = missing_perc[(missing_perc > 20) & (missing_perc <= 90)].index.tolist()\n",
    "low_missing_cols = missing_perc[missing_perc <= 20].index.tolist()\n",
    "\n",
    "print(\"Columns with >90% missing:\", high_missing_cols)\n",
    "print(\"Columns with 20-90% missing:\", moderate_missing_cols)\n",
    "print(\"Columns with <=20% missing:\", low_missing_cols)\n",
    "\n",
    "# Identify binary columns: we check numeric columns where the unique non-missing values are only 0 and 1.\n",
    "binary_cols = []\n",
    "for col in df.columns:\n",
    "    # Only check columns that can be treated as numeric (if not, skip this check)\n",
    "    try:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        # If the set of unique values is a subset of {0, 1}, then we consider it binary.\n",
    "        if set(unique_vals).issubset({0, 1}):\n",
    "            binary_cols.append(col)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "print(\"Identified binary columns:\", binary_cols)\n",
    "\n",
    "# For binary columns, fill missing values with 0\n",
    "df[binary_cols] = df[binary_cols].fillna(0)\n",
    "\n",
    "# For numeric columns that are not binary, impute missing values.\n",
    "# First, identify numeric columns (as numbers) and remove the binary ones.\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_binary_numeric_cols = list(set(numeric_cols) - set(binary_cols))\n",
    "\n",
    "# Impute non-binary numeric columns with median.\n",
    "# This is applied both to moderate and high missing rate columns.\n",
    "for col in non_binary_numeric_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# For categorical (non-numeric) columns, fill missing values with the mode.\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        mode_val = df[col].mode().iloc[0]\n",
    "        df[col] = df[col].fillna(mode_val)\n",
    "\n",
    "# ==========================\n",
    "# 4. Categorical Encoding\n",
    "# ==========================\n",
    "# At this stage, all columns have no missing values.\n",
    "# We now encode categorical variables to make the entire dataset numeric.\n",
    "# “Number of features remain consistent” means that every sample ends up with the same number of numeric columns.\n",
    "\n",
    "# Define a threshold to separate high-cardinality and low-cardinality categorical columns\n",
    "cardinality_threshold = 20\n",
    "high_card_cols = [col for col in categorical_cols if df[col].nunique() > cardinality_threshold]\n",
    "low_card_cols = [col for col in categorical_cols if df[col].nunique() <= cardinality_threshold]\n",
    "\n",
    "print(\"High-cardinality categorical columns:\", high_card_cols)\n",
    "print(\"Low-cardinality categorical columns:\", low_card_cols)\n",
    "\n",
    "# Frequency encoding for high-cardinality categorical columns:\n",
    "for col in high_card_cols:\n",
    "    freq_map = df[col].value_counts().to_dict()\n",
    "    df[col + \"_freq\"] = df[col].map(freq_map)\n",
    "\n",
    "# One-hot encoding for low-cardinality categorical columns:\n",
    "df = pd.get_dummies(df, columns=low_card_cols, drop_first=True)\n",
    "\n",
    "# ==========================\n",
    "# 5. Save the Preprocessed Dataset\n",
    "# ==========================\n",
    "# Now, the entire dataset is numeric with no missing values and a fixed set of features.\n",
    "# This dataset is ready for your GAN (without any dimensionality reduction).\n",
    "output_file = \"/Users/User2/Desktop/Research Seminar/processed_timegan_data_full.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"✅ Preprocessing complete! Processed dataset saved as '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7b7675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with >90% missing: ['bvorg', 'stblg', 'stjah', 'bktxt', 'bstat', 'grpid', 'xsnet', 'ausbk', 'stgrd', 'xreversal', 'augcp', 'augbl', 'umskz', 'umsks', 'txbhw', 'txbfw', 'hwbas', 'fwbas', 'mwart', 'sgtxt', 'vbund', 'kostl', 'vbeln', 'vbel2', 'posn2', 'xhres', 'xskrl', 'xauto', 'xzahl', 'zbd3t', 'zbd2p', 'sknto', 'wskto', 'zlspr', 'nebtr', 'mwsk1', 'dmbt1', 'dmbt2', 'rebzg', 'rebzj', 'rebzz', 'rebzt', 'landl', 'klibt', 'zekkn', 'bualt', 'rdiff', 'rdif2', 'txjcd', 'sknt2', 'hwmet', 'xragl', 'stbuk', 'txbh2', 'pprct', 'xnegp', 'kkber', 'agzei', 'auggj', 'segment', 'kstar']\n",
      "Columns with 20-90% missing: ['xblnr', 'kursf', 'xrueb', 'kurs2', 'xmwst', 'doccat', 'buzid', 'gsber', 'mwskz', 'txgrp', 'valut', 'zuonr', 'xumsw', 'xkres', 'xopvw', 'xncop', 'saknr', 'kunnr', 'lifnr', 'gvtyp', 'zfbdt', 'zterm', 'zbd1t', 'zbd2t', 'zbd1p', 'skfbt', 'matnr', 'werks', 'menge', 'meins', 'erfmg', 'erfme', 'bpmng', 'bprme', 'ebeln', 'ebelp', 'vprsv', 'bwkey', 'bustw', 'prctr', 'xref3', 'kidno', 'squan']\n",
      "Columns with <=20% missing: ['mandt', 'bukrs', 'belnr', 'gjahr', 'blart', 'monat', 'cputm', 'wwert', 'usnam', 'tcode', 'waers', 'glvor', 'awtyp', 'awkey', 'hwaer', 'hwae2', 'basw2', 'umrd2', 'curt2', 'kuty2', 'kurst', 'operation_flag_x', 'is_deleted_x', 'recordstamp_x', 'buzei', 'bschl', 'koart', 'shkzg', 'dmbtr', 'wrbtr', 'pswbt', 'pswsl', 'ktosl', 'vorgn', 'kokrs', 'hkont', 'xbilk', 'dmbe2', 'operation_flag_y', 'is_deleted_y', 'recordstamp_y', 'year', 'month', 'day', 'day_of_week', 'quarter', 'month_sin', 'month_cos', 'day_sin', 'day_cos']\n",
      "Identified binary columns: ['stgrd', 'is_deleted_x', 'rebzz', 'zekkn', 'is_deleted_y']\n",
      "High-cardinality categorical columns: ['cputm', 'wwert', 'usnam', 'xblnr', 'awkey', 'recordstamp_x', 'augcp', 'valut', 'zuonr', 'kunnr', 'lifnr', 'zfbdt', 'matnr', 'prctr', 'pprct', 'recordstamp_y']\n",
      "Low-cardinality categorical columns: ['bukrs', 'blart', 'tcode', 'bvorg', 'bktxt', 'waers', 'bstat', 'xrueb', 'glvor', 'grpid', 'awtyp', 'hwaer', 'hwae2', 'xmwst', 'kuty2', 'xsnet', 'ausbk', 'doccat', 'kurst', 'operation_flag_x', 'is_deleted_x', 'buzid', 'koart', 'umskz', 'umsks', 'shkzg', 'gsber', 'mwskz', 'pswsl', 'mwart', 'ktosl', 'sgtxt', 'vbund', 'vorgn', 'kokrs', 'kostl', 'xumsw', 'xhres', 'xkres', 'xopvw', 'xskrl', 'xauto', 'xncop', 'xzahl', 'xbilk', 'gvtyp', 'zlspr', 'mwsk1', 'rebzt', 'landl', 'meins', 'erfme', 'bprme', 'vprsv', 'bustw', 'txjcd', 'hwmet', 'xragl', 'stbuk', 'xnegp', 'kkber', 'segment', 'squan', 'operation_flag_y', 'is_deleted_y']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n",
      "/var/folders/qm/qt_g3_ls4hx6bc9jsnngjcw80000gq/T/ipykernel_85827/2829845349.py:96: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[col + \"_freq\"] = df[col].map(freq_map)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Preprocessing complete! Processed dataset saved as '/Users/User2/Desktop/Research Seminar/processed_timegan_data_full.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ==========================\n",
    "# 1. Load the Dataset\n",
    "# ==========================\n",
    "file_path = \"/Users/User2/Desktop/Research Seminar/approach_1_df.csv\"\n",
    "# Use low_memory=False to avoid dtype warnings\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# ==========================\n",
    "# 2. Date Conversion and Feature Extraction\n",
    "# ==========================\n",
    "# Define date columns; invalid dates (e.g. badly formatted strings) become NaT\n",
    "date_columns = [\"bldat\", \"budat\", \"cpudt\", \"aedat\", \"augdt\"]\n",
    "for col in date_columns:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "# Sort by posting date ('budat') to maintain time order\n",
    "df = df.sort_values(by=\"budat\")\n",
    "\n",
    "# Extract date features from 'budat'\n",
    "df[\"year\"] = df[\"budat\"].dt.year\n",
    "df[\"month\"] = df[\"budat\"].dt.month\n",
    "df[\"day\"] = df[\"budat\"].dt.day\n",
    "df[\"day_of_week\"] = df[\"budat\"].dt.weekday  # Monday=0, Sunday=6\n",
    "df[\"quarter\"] = df[\"budat\"].dt.quarter\n",
    "\n",
    "# Create cyclical (sin/cos) encoding for month and day-of-week\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
    "\n",
    "# Optionally drop the original date columns if you no longer need them\n",
    "df = df.drop(columns=date_columns)\n",
    "\n",
    "# ==========================\n",
    "# 3. Missing Value Analysis and Imputation\n",
    "# ==========================\n",
    "# Compute percentage of missing values per column\n",
    "missing_perc = df.isna().mean() * 100\n",
    "\n",
    "# Identify columns by missing value thresholds:\n",
    "high_missing_cols = missing_perc[missing_perc > 90].index.tolist()\n",
    "moderate_missing_cols = missing_perc[(missing_perc > 20) & (missing_perc <= 90)].index.tolist()\n",
    "low_missing_cols = missing_perc[missing_perc <= 20].index.tolist()\n",
    "\n",
    "print(\"Columns with >90% missing:\", high_missing_cols)\n",
    "print(\"Columns with 20-90% missing:\", moderate_missing_cols)\n",
    "print(\"Columns with <=20% missing:\", low_missing_cols)\n",
    "\n",
    "# Identify binary columns by checking if the unique non-missing values are only 0 and 1.\n",
    "binary_cols = []\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        unique_vals = df[col].dropna().unique()\n",
    "        if set(unique_vals).issubset({0, 1}):\n",
    "            binary_cols.append(col)\n",
    "    except Exception:\n",
    "        continue\n",
    "print(\"Identified binary columns:\", binary_cols)\n",
    "\n",
    "# Impute binary columns with 0 (if there are any missing values)\n",
    "df[binary_cols] = df[binary_cols].fillna(0)\n",
    "\n",
    "# For numeric (non-binary) columns, impute missing values with the median.\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "non_binary_numeric_cols = list(set(numeric_cols) - set(binary_cols))\n",
    "for col in non_binary_numeric_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# For categorical (non-numeric) columns, impute missing values with the mode.\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "for col in categorical_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df[col] = df[col].fillna(df[col].mode().iloc[0])\n",
    "\n",
    "# ==========================\n",
    "# 4. Categorical Encoding\n",
    "# ==========================\n",
    "# Define a cardinality threshold to separate high- and low-cardinality columns\n",
    "cardinality_threshold = 20\n",
    "high_card_cols = [col for col in categorical_cols if df[col].nunique() > cardinality_threshold]\n",
    "low_card_cols = [col for col in categorical_cols if df[col].nunique() <= cardinality_threshold]\n",
    "\n",
    "print(\"High-cardinality categorical columns:\", high_card_cols)\n",
    "print(\"Low-cardinality categorical columns:\", low_card_cols)\n",
    "\n",
    "# Frequency encoding for high-cardinality categorical columns\n",
    "# (Mapping each category to its frequency count)\n",
    "for col in high_card_cols:\n",
    "    freq_map = df[col].value_counts().to_dict()\n",
    "    df[col + \"_freq\"] = df[col].map(freq_map)\n",
    "\n",
    "# One-hot encoding for low-cardinality categorical columns\n",
    "df = pd.get_dummies(df, columns=low_card_cols, drop_first=True)\n",
    "\n",
    "# At this point, the entire dataset is numeric with no NaNs.\n",
    "# Save the preprocessed data before scaling (optional)\n",
    "preproc_output = \"/Users/User2/Desktop/Research Seminar/processed_timegan_data_full.csv\"\n",
    "df.to_csv(preproc_output, index=False)\n",
    "print(f\"✅ Preprocessing complete! Processed dataset saved as '{preproc_output}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae62fa6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '10:06:02'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Apply MinMax scaling to non-binary columns only.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m---> 14\u001b[0m df[non_binary_cols] \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df[non_binary_cols])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Save the final scaled dataset\u001b[39;00m\n\u001b[1;32m     17\u001b[0m scaled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/User2/Desktop/Research Seminar/timegan_minmax_scaled_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:434\u001b[0m, in \u001b[0;36mMinMaxScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:472\u001b[0m, in \u001b[0;36mMinMaxScaler.partial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMinMaxScaler does not support sparse input. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using MaxAbsScaler instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    469\u001b[0m     )\n\u001b[1;32m    471\u001b[0m first_pass \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 472\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m    473\u001b[0m     X,\n\u001b[1;32m    474\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_pass,\n\u001b[1;32m    475\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[1;32m    476\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    477\u001b[0m )\n\u001b[1;32m    479\u001b[0m data_min \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmin(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    480\u001b[0m data_max \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnanmax(X, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:838\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    837\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 838\u001b[0m     array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(new_dtype)\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    840\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6530\u001b[0m     results \u001b[38;5;241m=\u001b[39m [ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m   6532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6533\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6534\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mastype(dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   6535\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    412\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    416\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    417\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    418\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    419\u001b[0m     using_cow\u001b[38;5;241m=\u001b[39musing_copy_on_write(),\n\u001b[1;32m    420\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;124;03mBlock\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 616\u001b[0m new_values \u001b[38;5;241m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m    618\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    620\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m astype_array(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:183\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:134\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '10:06:02'"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 5. Scaling: MinMax Scaler for Non-Binary Columns\n",
    "# ==========================\n",
    "# Identify non-binary columns (all columns except those identified as binary)\n",
    "all_columns = df.columns.tolist()\n",
    "non_binary_cols = [col for col in all_columns if col not in binary_cols]\n",
    "\n",
    "# Fill missing values for binary columns only if they exist in the DataFrame.\n",
    "existing_binary_cols = [col for col in binary_cols if col in df.columns]\n",
    "df[existing_binary_cols] = df[existing_binary_cols].fillna(0)\n",
    "\n",
    "# Apply MinMax scaling to non-binary columns only.\n",
    "scaler = MinMaxScaler()\n",
    "df[non_binary_cols] = scaler.fit_transform(df[non_binary_cols])\n",
    "\n",
    "# Save the final scaled dataset\n",
    "scaled_output = \"/Users/User2/Desktop/Research Seminar/timegan_minmax_scaled_data.csv\"\n",
    "df.to_csv(scaled_output, index=False)\n",
    "print(f\"✅ MinMax scaling applied to non-binary columns. Final dataset saved as '{scaled_output}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f95cf049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded preprocessed dataset with shape: (334626, 285)\n",
      "✅ Columns: ['mandt', 'belnr', 'gjahr', 'monat', 'cputm', 'wwert', 'usnam', 'xblnr', 'stblg', 'stjah', 'kursf', 'awkey', 'kurs2', 'basw2', 'umrd2', 'curt2', 'stgrd', 'xreversal', 'recordstamp_x', 'buzei', 'augcp', 'augbl', 'bschl', 'dmbtr', 'wrbtr', 'pswbt', 'txbhw', 'txbfw', 'hwbas', 'fwbas', 'txgrp', 'valut', 'zuonr', 'vbeln', 'vbel2', 'posn2', 'saknr', 'hkont', 'kunnr', 'lifnr', 'zfbdt', 'zterm', 'zbd1t', 'zbd2t', 'zbd3t', 'zbd1p', 'zbd2p', 'skfbt', 'sknto', 'wskto', 'nebtr', 'dmbt1', 'dmbt2', 'rebzg', 'rebzj', 'rebzz', 'klibt', 'matnr', 'werks', 'menge', 'erfmg', 'bpmng', 'ebeln', 'ebelp', 'zekkn', 'bwkey', 'bualt', 'rdiff', 'rdif2', 'prctr', 'dmbe2', 'sknt2', 'txbh2', 'pprct', 'xref3', 'kidno', 'agzei', 'auggj', 'kstar', 'recordstamp_y', 'year', 'month', 'day', 'day_of_week', 'quarter', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'cputm_freq', 'wwert_freq', 'usnam_freq', 'xblnr_freq', 'awkey_freq', 'recordstamp_x_freq', 'augcp_freq', 'valut_freq', 'zuonr_freq', 'kunnr_freq', 'lifnr_freq', 'zfbdt_freq', 'matnr_freq', 'prctr_freq', 'pprct_freq', 'recordstamp_y_freq', 'bukrs_C002', 'bukrs_C003', 'bukrs_C004', 'bukrs_C005', 'bukrs_EU01', 'bukrs_USA1', 'blart_DA', 'blart_DR', 'blart_DZ', 'blart_KG', 'blart_KR', 'blart_KZ', 'blart_PR', 'blart_RE', 'blart_RV', 'blart_SA', 'blart_WA', 'blart_WE', 'blart_WL', 'tcode_FB08', 'tcode_FB1D', 'tcode_FB60', 'tcode_FB70', 'tcode_FBM1', 'tcode_FBZ1', 'tcode_FBZ2', 'tcode_MB01', 'tcode_MB1C', 'tcode_MIGO_GI', 'tcode_MIGO_GR', 'tcode_MIRO', 'tcode_MR21', 'tcode_MR8M', 'tcode_VF01', 'tcode_VF02', 'tcode_VF11', 'tcode_VL02N', 'tcode_VL09', 'bvorg_1900000001C00222', 'bvorg_1900000002C00222', 'bktxt_Initial stock upload', 'waers_CNY', 'waers_EUR', 'waers_JPY', 'waers_USD', 'glvor_RFIG', 'glvor_RMPR', 'glvor_RMRP', 'glvor_RMWA', 'glvor_RMWE', 'glvor_RMWL', 'glvor_SD00', 'grpid_IVUS02', 'awtyp_IBKPF', 'awtyp_MKPF', 'awtyp_PRCHG', 'awtyp_RMRP', 'awtyp_VBRK', 'hwaer_EUR', 'hwaer_JPY', 'hwaer_USD', 'operation_flag_x_L', 'operation_flag_x_U', 'buzid_P', 'buzid_S', 'buzid_T', 'buzid_U', 'buzid_W', 'koart_K', 'koart_M', 'koart_S', 'shkzg_S', 'gsber_1', 'gsber_1.0', 'gsber_EU01', 'gsber_PACK', 'gsber_USA1', 'mwskz_A1', 'mwskz_A9', 'mwskz_I0', 'mwskz_I1', 'mwskz_O0', 'mwskz_O1', 'mwskz_P1', 'mwskz_S1', 'mwskz_V0', 'mwskz_V1', 'mwskz_V2', 'pswsl_CNY', 'pswsl_EUR', 'pswsl_JPY', 'pswsl_USD', 'mwart_V', 'ktosl_AUM', 'ktosl_BSV', 'ktosl_BSX', 'ktosl_BUV', 'ktosl_EGK', 'ktosl_FRL', 'ktosl_GBB', 'ktosl_KBS', 'ktosl_KDF', 'ktosl_KDM', 'ktosl_MW1', 'ktosl_MW5', 'ktosl_MWS', 'ktosl_PRD', 'ktosl_SKT', 'ktosl_UMB', 'ktosl_VS1', 'ktosl_VST', 'ktosl_WRX', 'sgtxt_Debit/credit to a material from a price change', 'sgtxt_Incoming Payment', 'sgtxt_MR8M', 'sgtxt_Revaluation from a price change', 'sgtxt_Test Payment', 'sgtxt_adding stock for Backorder test', 'vbund_C002', 'vorgn_RFIG', 'vorgn_RMPR', 'vorgn_RMRP', 'vorgn_RMWA', 'vorgn_RMWE', 'vorgn_RMWL', 'vorgn_SD00', 'kokrs_CYMB', 'kostl_AP016000', 'kostl_CA017000', 'kostl_EU014000', 'kostl_EU016000', 'kostl_US011150', 'kostl_US011770', 'kostl_US014000', 'kostl_US015000', 'kostl_US016000', 'kostl_US017000', 'kostl_US024000', 'mwsk1_O1', 'landl_CA', 'landl_CN', 'landl_DE', 'landl_IE', 'landl_NL', 'meins_CS', 'meins_EA', 'meins_G', 'meins_KAR', 'meins_KG', 'meins_L', 'meins_LB', 'erfme_CS', 'erfme_EA', 'erfme_G', 'erfme_KAR', 'erfme_KG', 'erfme_L', 'erfme_LB', 'bprme_G', 'bprme_KG', 'bprme_L', 'bprme_LB', 'vprsv_V', 'bustw_RE02', 'bustw_RE14', 'bustw_RE21', 'bustw_WA01', 'bustw_WA04', 'bustw_WE01', 'txjcd_CA0000000', 'txjcd_CANS', 'txjcd_CAON', 'txjcd_GA0000000', 'hwmet_E', 'kkber_EU01', 'kkber_USA1', 'squan_-', 'squan_0', 'operation_flag_y_L', 'operation_flag_y_U']\n",
      "✅ Identified binary columns: ['stgrd', 'rebzz', 'zekkn', 'bukrs_C002', 'bukrs_C003', 'bukrs_C004', 'bukrs_C005', 'bukrs_EU01', 'bukrs_USA1', 'blart_DA', 'blart_DR', 'blart_DZ', 'blart_KG', 'blart_KR', 'blart_KZ', 'blart_PR', 'blart_RE', 'blart_RV', 'blart_SA', 'blart_WA', 'blart_WE', 'blart_WL', 'tcode_FB08', 'tcode_FB1D', 'tcode_FB60', 'tcode_FB70', 'tcode_FBM1', 'tcode_FBZ1', 'tcode_FBZ2', 'tcode_MB01', 'tcode_MB1C', 'tcode_MIGO_GI', 'tcode_MIGO_GR', 'tcode_MIRO', 'tcode_MR21', 'tcode_MR8M', 'tcode_VF01', 'tcode_VF02', 'tcode_VF11', 'tcode_VL02N', 'tcode_VL09', 'bvorg_1900000001C00222', 'bvorg_1900000002C00222', 'bktxt_Initial stock upload', 'waers_CNY', 'waers_EUR', 'waers_JPY', 'waers_USD', 'glvor_RFIG', 'glvor_RMPR', 'glvor_RMRP', 'glvor_RMWA', 'glvor_RMWE', 'glvor_RMWL', 'glvor_SD00', 'grpid_IVUS02', 'awtyp_IBKPF', 'awtyp_MKPF', 'awtyp_PRCHG', 'awtyp_RMRP', 'awtyp_VBRK', 'hwaer_EUR', 'hwaer_JPY', 'hwaer_USD', 'operation_flag_x_L', 'operation_flag_x_U', 'buzid_P', 'buzid_S', 'buzid_T', 'buzid_U', 'buzid_W', 'koart_K', 'koart_M', 'koart_S', 'shkzg_S', 'gsber_1', 'gsber_1.0', 'gsber_EU01', 'gsber_PACK', 'gsber_USA1', 'mwskz_A1', 'mwskz_A9', 'mwskz_I0', 'mwskz_I1', 'mwskz_O0', 'mwskz_O1', 'mwskz_P1', 'mwskz_S1', 'mwskz_V0', 'mwskz_V1', 'mwskz_V2', 'pswsl_CNY', 'pswsl_EUR', 'pswsl_JPY', 'pswsl_USD', 'mwart_V', 'ktosl_AUM', 'ktosl_BSV', 'ktosl_BSX', 'ktosl_BUV', 'ktosl_EGK', 'ktosl_FRL', 'ktosl_GBB', 'ktosl_KBS', 'ktosl_KDF', 'ktosl_KDM', 'ktosl_MW1', 'ktosl_MW5', 'ktosl_MWS', 'ktosl_PRD', 'ktosl_SKT', 'ktosl_UMB', 'ktosl_VS1', 'ktosl_VST', 'ktosl_WRX', 'sgtxt_Debit/credit to a material from a price change', 'sgtxt_Incoming Payment', 'sgtxt_MR8M', 'sgtxt_Revaluation from a price change', 'sgtxt_Test Payment', 'sgtxt_adding stock for Backorder test', 'vbund_C002', 'vorgn_RFIG', 'vorgn_RMPR', 'vorgn_RMRP', 'vorgn_RMWA', 'vorgn_RMWE', 'vorgn_RMWL', 'vorgn_SD00', 'kokrs_CYMB', 'kostl_AP016000', 'kostl_CA017000', 'kostl_EU014000', 'kostl_EU016000', 'kostl_US011150', 'kostl_US011770', 'kostl_US014000', 'kostl_US015000', 'kostl_US016000', 'kostl_US017000', 'kostl_US024000', 'mwsk1_O1', 'landl_CA', 'landl_CN', 'landl_DE', 'landl_IE', 'landl_NL', 'meins_CS', 'meins_EA', 'meins_G', 'meins_KAR', 'meins_KG', 'meins_L', 'meins_LB', 'erfme_CS', 'erfme_EA', 'erfme_G', 'erfme_KAR', 'erfme_KG', 'erfme_L', 'erfme_LB', 'bprme_G', 'bprme_KG', 'bprme_L', 'bprme_LB', 'vprsv_V', 'bustw_RE02', 'bustw_RE14', 'bustw_RE21', 'bustw_WA01', 'bustw_WA04', 'bustw_WE01', 'txjcd_CA0000000', 'txjcd_CANS', 'txjcd_CAON', 'txjcd_GA0000000', 'hwmet_E', 'kkber_EU01', 'kkber_USA1', 'squan_-', 'squan_0', 'operation_flag_y_L', 'operation_flag_y_U']\n",
      "✅ Non-binary numeric columns (will be scaled): ['mandt', 'belnr', 'gjahr', 'monat', 'cputm', 'stblg', 'stjah', 'kursf', 'kurs2', 'basw2', 'umrd2', 'curt2', 'xreversal', 'buzei', 'augbl', 'bschl', 'dmbtr', 'wrbtr', 'pswbt', 'txbhw', 'txbfw', 'hwbas', 'fwbas', 'txgrp', 'vbeln', 'vbel2', 'posn2', 'saknr', 'hkont', 'zterm', 'zbd1t', 'zbd2t', 'zbd3t', 'zbd1p', 'zbd2p', 'skfbt', 'sknto', 'wskto', 'nebtr', 'dmbt1', 'dmbt2', 'rebzg', 'rebzj', 'klibt', 'werks', 'menge', 'erfmg', 'bpmng', 'ebeln', 'ebelp', 'bwkey', 'bualt', 'rdiff', 'rdif2', 'dmbe2', 'sknt2', 'txbh2', 'xref3', 'kidno', 'agzei', 'auggj', 'kstar', 'year', 'month', 'day', 'day_of_week', 'quarter', 'month_sin', 'month_cos', 'day_sin', 'day_cos', 'cputm_freq', 'wwert_freq', 'usnam_freq', 'xblnr_freq', 'awkey_freq', 'recordstamp_x_freq', 'augcp_freq', 'valut_freq', 'zuonr_freq', 'kunnr_freq', 'lifnr_freq', 'zfbdt_freq', 'matnr_freq', 'prctr_freq', 'pprct_freq', 'recordstamp_y_freq']\n",
      "✅ All done! Final scaled dataset saved at '/Users/User2/Desktop/Research Seminar/timegan_fully_scaled_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# ==========================\n",
    "# 1. Load Preprocessed Dataset\n",
    "# ==========================\n",
    "preprocessed_file = \"/Users/User2/Desktop/Research Seminar/processed_timegan_data_full.csv\"\n",
    "df = pd.read_csv(preprocessed_file, low_memory=False)\n",
    "\n",
    "print(\"✅ Loaded preprocessed dataset with shape:\", df.shape)\n",
    "print(\"✅ Columns:\", df.columns.tolist())\n",
    "\n",
    "# ==========================\n",
    "# 2. Convert Time-Like String Columns to Numeric (Seconds)\n",
    "# ==========================\n",
    "# If you know which columns contain time-like strings (e.g., \"HH:MM:SS\"),\n",
    "# list them here. For example, 'cputm' might have values like \"10:06:02\".\n",
    "time_like_cols = [\"cputm\"]  # <-- Adjust this list for your actual time-based columns\n",
    "\n",
    "for col in time_like_cols:\n",
    "    if col in df.columns:\n",
    "        # Convert string \"HH:MM:SS\" to pandas datetime (coerce invalid formats to NaT)\n",
    "        df[col] = pd.to_datetime(df[col], format=\"%H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "        # If you prefer, fill NaT (missing after conversion) with 0:0:0\n",
    "        df[col] = df[col].fillna(pd.to_datetime(\"00:00:00\"))\n",
    "\n",
    "        # Convert to total seconds: hour*3600 + minute*60 + second\n",
    "        df[col] = df[col].dt.hour * 3600 + df[col].dt.minute * 60 + df[col].dt.second\n",
    "\n",
    "# ==========================\n",
    "# 3. Identify Binary Columns\n",
    "# ==========================\n",
    "# We'll check numeric columns whose unique non-missing values are a subset of {0, 1}.\n",
    "binary_cols = []\n",
    "for col in df.columns:\n",
    "    # Only consider columns that are numeric; skip if object/string\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        unique_vals = set(df[col].dropna().unique())\n",
    "        if unique_vals.issubset({0, 1}):\n",
    "            binary_cols.append(col)\n",
    "\n",
    "print(\"✅ Identified binary columns:\", binary_cols)\n",
    "\n",
    "# ==========================\n",
    "# 4. Select Numeric, Non-Binary Columns for MinMax Scaling\n",
    "# ==========================\n",
    "non_binary_numeric_cols = []\n",
    "for col in df.columns:\n",
    "    # We only scale columns that are numeric and not in binary_cols\n",
    "    if pd.api.types.is_numeric_dtype(df[col]) and col not in binary_cols:\n",
    "        non_binary_numeric_cols.append(col)\n",
    "\n",
    "print(\"✅ Non-binary numeric columns (will be scaled):\", non_binary_numeric_cols)\n",
    "\n",
    "# ==========================\n",
    "# 5. Apply MinMax Scaler to Non-Binary Numeric Columns\n",
    "# ==========================\n",
    "scaler = MinMaxScaler()\n",
    "df[non_binary_numeric_cols] = scaler.fit_transform(df[non_binary_numeric_cols])\n",
    "\n",
    "# ==========================\n",
    "# 6. Save the Fully Scaled Dataset\n",
    "# ==========================\n",
    "scaled_output = \"/Users/User2/Desktop/Research Seminar/timegan_fully_scaled_data.csv\"\n",
    "df.to_csv(scaled_output, index=False)\n",
    "print(f\"✅ All done! Final scaled dataset saved at '{scaled_output}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157ab320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.9.1 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.9.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.9.1  # or later 2.x version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b52926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 02:00:30.920471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0bfe8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (N, D): (334626, 285)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2018-01-25'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m     seq_slice \u001b[38;5;241m=\u001b[39m data[i : i \u001b[38;5;241m+\u001b[39m seq_len, :]\n\u001b[1;32m     21\u001b[0m     sequences\u001b[38;5;241m.\u001b[39mappend(seq_slice)\n\u001b[0;32m---> 23\u001b[0m sequences \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(sequences, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# shape: (N - seq_len + 1, seq_len, D)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequences shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequences\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Optionally, split into training and testing for final evaluation\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2018-01-25'"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Load the Scaled Dataset\n",
    "# ==========================\n",
    "data_file = \"/Users/User2/Desktop/Research Seminar/timegan_fully_scaled_data.csv\"\n",
    "df = pd.read_csv(data_file, low_memory=False)\n",
    "print(\"Data shape (N, D):\", df.shape)\n",
    "\n",
    "# Convert to numpy\n",
    "data = df.values  # shape: (N, D)\n",
    "\n",
    "# ==========================\n",
    "# Define Sequence Length\n",
    "# ==========================\n",
    "seq_len = 30  # adjust as appropriate for your domain\n",
    "dim = data.shape[1]  # number of features (D)\n",
    "\n",
    "# Create overlapping windows of length seq_len\n",
    "sequences = []\n",
    "for i in range(len(data) - seq_len + 1):\n",
    "    seq_slice = data[i : i + seq_len, :]\n",
    "    sequences.append(seq_slice)\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32)  # shape: (N - seq_len + 1, seq_len, D)\n",
    "print(\"Sequences shape:\", sequences.shape)\n",
    "\n",
    "# Optionally, split into training and testing for final evaluation\n",
    "train_data, test_data = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "print(\"Train shape:\", train_data.shape, \"| Test shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00a961ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mandt                 float64\n",
      "belnr                 float64\n",
      "gjahr                 float64\n",
      "monat                 float64\n",
      "cputm                 float64\n",
      "                       ...   \n",
      "kkber_USA1               bool\n",
      "squan_-                  bool\n",
      "squan_0                  bool\n",
      "operation_flag_y_L       bool\n",
      "operation_flag_y_U       bool\n",
      "Length: 285, dtype: object\n",
      "Non-numeric columns: Index(['wwert', 'usnam', 'xblnr', 'awkey', 'recordstamp_x', 'augcp', 'valut',\n",
      "       'zuonr', 'kunnr', 'lifnr',\n",
      "       ...\n",
      "       'txjcd_CANS', 'txjcd_CAON', 'txjcd_GA0000000', 'hwmet_E', 'kkber_EU01',\n",
      "       'kkber_USA1', 'squan_-', 'squan_0', 'operation_flag_y_L',\n",
      "       'operation_flag_y_U'],\n",
      "      dtype='object', length=195)\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n",
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eda2c4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in wwert: ['2018-01-25' '2018-04-04' '2018-04-06' '2018-04-07' '2018-04-08'\n",
      " '2018-04-09' '2018-04-10' '2018-04-11' '2018-04-12' '2018-04-13']\n",
      "Unique values in usnam: ['SHOBHITSAURA' 'SIBASISM' 'MOUNIKAPALI' 'BHUSHAN' 'GAIKWADPAWAR'\n",
      " 'VISHAKHA' 'RAJARSHIG' 'PKADARI' 'ANANTHARAMUL' 'MALVIYAH']\n",
      "Unique values in xblnr: ['TEST' '0080000079' '0080000080' '0080000081' '0080000075' '0080000074'\n",
      " '0080000076' '0080000077' '0080000078' '0080000000']\n",
      "Unique values in awkey: ['1900000000USA12018' '50000325312018' '50000326792018' '50000325372018'\n",
      " '50000329072018' '50000329622018' '51056450872018' '51056304442018'\n",
      " '51056304452018' '50000477702018']\n",
      "Unique values in recordstamp_x: ['2022-01-31 20:41:45.693184+00:00' '2022-03-22 05:40:44.237359+00:00'\n",
      " '2022-03-22 05:41:07.064797+00:00' '2022-03-22 05:40:45.067314+00:00'\n",
      " '2022-03-22 05:41:40.700059+00:00' '2022-03-22 05:41:50.078926+00:00'\n",
      " '2022-03-26 02:44:48.288650+00:00' '2022-03-23 08:25:31.925815+00:00'\n",
      " '2022-03-22 10:15:32.579313+00:00' '2022-03-22 05:41:45.175226+00:00']\n",
      "Unique values in augcp: ['2022-03-28' '2022-01-23' '2022-02-14' '2022-01-27' '2022-01-17'\n",
      " '2022-01-25' '2022-02-17' '2022-02-12' '2022-02-16' '2022-02-01']\n",
      "Unique values in valut: ['2022-03-10' '2022-03-21' '2022-03-23' '2022-03-22' '2022-03-02'\n",
      " '2022-03-11' '2022-02-03' '2022-02-04' '2022-02-10' '2022-02-22']\n",
      "Unique values in zuonr: ['20220328' '450003920300010' '450003769300010' '450003915500010'\n",
      " '450005081700010' '450005121400010' '450006599400010.0'\n",
      " '450003769300010.0' '450006674700010' '450006599400010']\n",
      "Unique values in kunnr: ['1000038' '2.0' '1000000.0' '1000001.0' '1000000' '1000011.0' '1000012.0'\n",
      " '1000013.0' '1000010.0' '1000002.0']\n",
      "Unique values in lifnr: ['916521' 'TEST' '288395' '0000447487' '447487' '0000252004' '623106'\n",
      " '68739' '0000068739' '252004']\n",
      "Unique values in zfbdt: ['2019-08-28' '2018-01-25' '2018-04-04' '2018-04-06' '2018-04-07'\n",
      " '2018-04-08' '2018-04-09' '2018-04-10' '2018-04-11' '2018-04-12']\n",
      "Unique values in matnr: ['C0000077' '6666666719' '000000006666666727' '6666666727'\n",
      " '000000006666666714' '6666666714' '000000006666666715' '6666666718'\n",
      " '6666666715' '6666666690']\n",
      "Unique values in prctr: ['US01ELHC10' 'US01SALES' 'US01ADMIN' 'US01PGHP02' 'US01ELHC09'\n",
      " 'US01PGHP01' 'US01PSFD01' 'US01PSFD02' 'US02PGHP01' 'US02PGHP02']\n",
      "Unique values in pprct: ['US01ELHE01' 'US01SALES' 'US01PGHP02' 'US01PSFD01' 'US01PSFD02'\n",
      " 'US01ELHC09' 'US01ELHC10' 'US01PGHP01' 'US02ELHC10' 'US02ELHC09']\n",
      "Unique values in recordstamp_y: ['2022-01-31 20:41:47.268774+00:00' '2022-03-22 05:40:58.289463+00:00'\n",
      " '2022-03-22 05:41:07.791593+00:00' '2022-03-22 05:41:40.982680+00:00'\n",
      " '2022-03-22 05:41:49.243168+00:00' '2022-03-26 02:44:58.721566+00:00'\n",
      " '2022-03-23 07:26:33.586655+00:00' '2022-03-23 07:26:34.018495+00:00'\n",
      " '2022-03-22 09:15:26.972498+00:00' '2022-03-22 09:16:08.364171+00:00']\n",
      "Unique values in bukrs_C002: [False  True]\n",
      "Unique values in bukrs_C003: [False  True]\n",
      "Unique values in bukrs_C004: [False  True]\n",
      "Unique values in bukrs_C005: [False  True]\n",
      "Unique values in bukrs_EU01: [False  True]\n",
      "Unique values in bukrs_USA1: [ True False]\n",
      "Unique values in blart_DA: [False  True]\n",
      "Unique values in blart_DR: [False  True]\n",
      "Unique values in blart_DZ: [False  True]\n",
      "Unique values in blart_KG: [False  True]\n",
      "Unique values in blart_KR: [ True False]\n",
      "Unique values in blart_KZ: [False  True]\n",
      "Unique values in blart_PR: [False  True]\n",
      "Unique values in blart_RE: [False  True]\n",
      "Unique values in blart_RV: [False  True]\n",
      "Unique values in blart_SA: [False  True]\n",
      "Unique values in blart_WA: [False  True]\n",
      "Unique values in blart_WE: [False  True]\n",
      "Unique values in blart_WL: [False  True]\n",
      "Unique values in tcode_FB08: [False  True]\n",
      "Unique values in tcode_FB1D: [False  True]\n",
      "Unique values in tcode_FB60: [ True False]\n",
      "Unique values in tcode_FB70: [False  True]\n",
      "Unique values in tcode_FBM1: [False  True]\n",
      "Unique values in tcode_FBZ1: [False  True]\n",
      "Unique values in tcode_FBZ2: [False  True]\n",
      "Unique values in tcode_MB01: [False  True]\n",
      "Unique values in tcode_MB1C: [False  True]\n",
      "Unique values in tcode_MIGO_GI: [False  True]\n",
      "Unique values in tcode_MIGO_GR: [False  True]\n",
      "Unique values in tcode_MIRO: [False  True]\n",
      "Unique values in tcode_MR21: [False  True]\n",
      "Unique values in tcode_MR8M: [False  True]\n",
      "Unique values in tcode_VF01: [False  True]\n",
      "Unique values in tcode_VF02: [False  True]\n",
      "Unique values in tcode_VF11: [False  True]\n",
      "Unique values in tcode_VL02N: [False  True]\n",
      "Unique values in tcode_VL09: [False  True]\n",
      "Unique values in bvorg_1900000001C00222: [False  True]\n",
      "Unique values in bvorg_1900000002C00222: [False  True]\n",
      "Unique values in bktxt_Initial stock upload: [ True False]\n",
      "Unique values in waers_CNY: [False  True]\n",
      "Unique values in waers_EUR: [False  True]\n",
      "Unique values in waers_JPY: [False  True]\n",
      "Unique values in waers_USD: [ True False]\n",
      "Unique values in glvor_RFIG: [False  True]\n",
      "Unique values in glvor_RMPR: [False  True]\n",
      "Unique values in glvor_RMRP: [False  True]\n",
      "Unique values in glvor_RMWA: [False  True]\n",
      "Unique values in glvor_RMWE: [False  True]\n",
      "Unique values in glvor_RMWL: [False  True]\n",
      "Unique values in glvor_SD00: [False  True]\n",
      "Unique values in grpid_IVUS02: [ True False]\n",
      "Unique values in awtyp_IBKPF: [False  True]\n",
      "Unique values in awtyp_MKPF: [False  True]\n",
      "Unique values in awtyp_PRCHG: [False  True]\n",
      "Unique values in awtyp_RMRP: [False  True]\n",
      "Unique values in awtyp_VBRK: [False  True]\n",
      "Unique values in hwaer_EUR: [False  True]\n",
      "Unique values in hwaer_JPY: [False  True]\n",
      "Unique values in hwaer_USD: [ True False]\n",
      "Unique values in operation_flag_x_L: [ True False]\n",
      "Unique values in operation_flag_x_U: [False  True]\n",
      "Unique values in buzid_P: [False  True]\n",
      "Unique values in buzid_S: [False  True]\n",
      "Unique values in buzid_T: [False  True]\n",
      "Unique values in buzid_U: [False  True]\n",
      "Unique values in buzid_W: [ True False]\n",
      "Unique values in koart_K: [False  True]\n",
      "Unique values in koart_M: [False  True]\n",
      "Unique values in koart_S: [ True False]\n",
      "Unique values in shkzg_S: [ True False]\n",
      "Unique values in gsber_1: [ True False]\n",
      "Unique values in gsber_1.0: [False  True]\n",
      "Unique values in gsber_EU01: [False  True]\n",
      "Unique values in gsber_PACK: [False  True]\n",
      "Unique values in gsber_USA1: [False  True]\n",
      "Unique values in mwskz_A1: [False  True]\n",
      "Unique values in mwskz_A9: [False  True]\n",
      "Unique values in mwskz_I0: [ True False]\n",
      "Unique values in mwskz_I1: [False  True]\n",
      "Unique values in mwskz_O0: [False  True]\n",
      "Unique values in mwskz_O1: [False  True]\n",
      "Unique values in mwskz_P1: [False  True]\n",
      "Unique values in mwskz_S1: [False  True]\n",
      "Unique values in mwskz_V0: [False  True]\n",
      "Unique values in mwskz_V1: [False  True]\n",
      "Unique values in mwskz_V2: [False  True]\n",
      "Unique values in pswsl_CNY: [False  True]\n",
      "Unique values in pswsl_EUR: [False  True]\n",
      "Unique values in pswsl_JPY: [False  True]\n",
      "Unique values in pswsl_USD: [ True False]\n",
      "Unique values in mwart_V: [False  True]\n",
      "Unique values in ktosl_AUM: [False  True]\n",
      "Unique values in ktosl_BSV: [False  True]\n",
      "Unique values in ktosl_BSX: [False  True]\n",
      "Unique values in ktosl_BUV: [False  True]\n",
      "Unique values in ktosl_EGK: [False  True]\n",
      "Unique values in ktosl_FRL: [False  True]\n",
      "Unique values in ktosl_GBB: [False  True]\n",
      "Unique values in ktosl_KBS: [False  True]\n",
      "Unique values in ktosl_KDF: [False  True]\n",
      "Unique values in ktosl_KDM: [False  True]\n",
      "Unique values in ktosl_MW1: [False  True]\n",
      "Unique values in ktosl_MW5: [False  True]\n",
      "Unique values in ktosl_MWS: [False  True]\n",
      "Unique values in ktosl_PRD: [False  True]\n",
      "Unique values in ktosl_SKT: [False  True]\n",
      "Unique values in ktosl_UMB: [False  True]\n",
      "Unique values in ktosl_VS1: [False  True]\n",
      "Unique values in ktosl_VST: [False  True]\n",
      "Unique values in ktosl_WRX: [ True False]\n",
      "Unique values in sgtxt_Debit/credit to a material from a price change: [False  True]\n",
      "Unique values in sgtxt_Incoming Payment: [ True False]\n",
      "Unique values in sgtxt_MR8M: [False  True]\n",
      "Unique values in sgtxt_Revaluation from a price change: [False  True]\n",
      "Unique values in sgtxt_Test Payment: [False  True]\n",
      "Unique values in sgtxt_adding stock for Backorder test: [False  True]\n",
      "Unique values in vbund_C002: [False  True]\n",
      "Unique values in vorgn_RFIG: [False  True]\n",
      "Unique values in vorgn_RMPR: [False  True]\n",
      "Unique values in vorgn_RMRP: [False  True]\n",
      "Unique values in vorgn_RMWA: [False  True]\n",
      "Unique values in vorgn_RMWE: [False  True]\n",
      "Unique values in vorgn_RMWL: [False  True]\n",
      "Unique values in vorgn_SD00: [False  True]\n",
      "Unique values in kokrs_CYMB: [ True False]\n",
      "Unique values in kostl_AP016000: [False  True]\n",
      "Unique values in kostl_CA017000: [False  True]\n",
      "Unique values in kostl_EU014000: [False  True]\n",
      "Unique values in kostl_EU016000: [False  True]\n",
      "Unique values in kostl_US011150: [False  True]\n",
      "Unique values in kostl_US011770: [False  True]\n",
      "Unique values in kostl_US014000: [ True False]\n",
      "Unique values in kostl_US015000: [False  True]\n",
      "Unique values in kostl_US016000: [False  True]\n",
      "Unique values in kostl_US017000: [False  True]\n",
      "Unique values in kostl_US024000: [False  True]\n",
      "Unique values in mwsk1_O1: [False  True]\n",
      "Unique values in landl_CA: [False  True]\n",
      "Unique values in landl_CN: [False  True]\n",
      "Unique values in landl_DE: [False  True]\n",
      "Unique values in landl_IE: [False  True]\n",
      "Unique values in landl_NL: [ True False]\n",
      "Unique values in meins_CS: [False  True]\n",
      "Unique values in meins_EA: [False  True]\n",
      "Unique values in meins_G: [False  True]\n",
      "Unique values in meins_KAR: [False  True]\n",
      "Unique values in meins_KG: [ True False]\n",
      "Unique values in meins_L: [False  True]\n",
      "Unique values in meins_LB: [False  True]\n",
      "Unique values in erfme_CS: [False  True]\n",
      "Unique values in erfme_EA: [False  True]\n",
      "Unique values in erfme_G: [False  True]\n",
      "Unique values in erfme_KAR: [False  True]\n",
      "Unique values in erfme_KG: [ True False]\n",
      "Unique values in erfme_L: [False  True]\n",
      "Unique values in erfme_LB: [False  True]\n",
      "Unique values in bprme_G: [False  True]\n",
      "Unique values in bprme_KG: [ True False]\n",
      "Unique values in bprme_L: [False  True]\n",
      "Unique values in bprme_LB: [False  True]\n",
      "Unique values in vprsv_V: [ True False]\n",
      "Unique values in bustw_RE02: [False  True]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in bustw_RE14: [False  True]\n",
      "Unique values in bustw_RE21: [False  True]\n",
      "Unique values in bustw_WA01: [False  True]\n",
      "Unique values in bustw_WA04: [False  True]\n",
      "Unique values in bustw_WE01: [ True False]\n",
      "Unique values in txjcd_CA0000000: [ True False]\n",
      "Unique values in txjcd_CANS: [False  True]\n",
      "Unique values in txjcd_CAON: [False  True]\n",
      "Unique values in txjcd_GA0000000: [False  True]\n",
      "Unique values in hwmet_E: [ True False]\n",
      "Unique values in kkber_EU01: [False  True]\n",
      "Unique values in kkber_USA1: [False  True]\n",
      "Unique values in squan_-: [ True False]\n",
      "Unique values in squan_0: [False  True]\n",
      "Unique values in operation_flag_y_L: [ True False]\n",
      "Unique values in operation_flag_y_U: [False  True]\n"
     ]
    }
   ],
   "source": [
    "for col in non_numeric_cols:\n",
    "    print(f\"Unique values in {col}:\", df[col].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "303400d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data Shape: (334626, 285)\n",
      "String/object columns: ['wwert', 'usnam', 'xblnr', 'awkey', 'recordstamp_x', 'augcp', 'valut', 'zuonr', 'kunnr', 'lifnr', 'zfbdt', 'matnr', 'prctr', 'pprct', 'recordstamp_y']\n",
      "Converting boolean columns to int: ['bukrs_C002', 'bukrs_C003', 'bukrs_C004', 'bukrs_C005', 'bukrs_EU01', 'bukrs_USA1', 'blart_DA', 'blart_DR', 'blart_DZ', 'blart_KG', 'blart_KR', 'blart_KZ', 'blart_PR', 'blart_RE', 'blart_RV', 'blart_SA', 'blart_WA', 'blart_WE', 'blart_WL', 'tcode_FB08', 'tcode_FB1D', 'tcode_FB60', 'tcode_FB70', 'tcode_FBM1', 'tcode_FBZ1', 'tcode_FBZ2', 'tcode_MB01', 'tcode_MB1C', 'tcode_MIGO_GI', 'tcode_MIGO_GR', 'tcode_MIRO', 'tcode_MR21', 'tcode_MR8M', 'tcode_VF01', 'tcode_VF02', 'tcode_VF11', 'tcode_VL02N', 'tcode_VL09', 'bvorg_1900000001C00222', 'bvorg_1900000002C00222', 'bktxt_Initial stock upload', 'waers_CNY', 'waers_EUR', 'waers_JPY', 'waers_USD', 'glvor_RFIG', 'glvor_RMPR', 'glvor_RMRP', 'glvor_RMWA', 'glvor_RMWE', 'glvor_RMWL', 'glvor_SD00', 'grpid_IVUS02', 'awtyp_IBKPF', 'awtyp_MKPF', 'awtyp_PRCHG', 'awtyp_RMRP', 'awtyp_VBRK', 'hwaer_EUR', 'hwaer_JPY', 'hwaer_USD', 'operation_flag_x_L', 'operation_flag_x_U', 'buzid_P', 'buzid_S', 'buzid_T', 'buzid_U', 'buzid_W', 'koart_K', 'koart_M', 'koart_S', 'shkzg_S', 'gsber_1', 'gsber_1.0', 'gsber_EU01', 'gsber_PACK', 'gsber_USA1', 'mwskz_A1', 'mwskz_A9', 'mwskz_I0', 'mwskz_I1', 'mwskz_O0', 'mwskz_O1', 'mwskz_P1', 'mwskz_S1', 'mwskz_V0', 'mwskz_V1', 'mwskz_V2', 'pswsl_CNY', 'pswsl_EUR', 'pswsl_JPY', 'pswsl_USD', 'mwart_V', 'ktosl_AUM', 'ktosl_BSV', 'ktosl_BSX', 'ktosl_BUV', 'ktosl_EGK', 'ktosl_FRL', 'ktosl_GBB', 'ktosl_KBS', 'ktosl_KDF', 'ktosl_KDM', 'ktosl_MW1', 'ktosl_MW5', 'ktosl_MWS', 'ktosl_PRD', 'ktosl_SKT', 'ktosl_UMB', 'ktosl_VS1', 'ktosl_VST', 'ktosl_WRX', 'sgtxt_Debit/credit to a material from a price change', 'sgtxt_Incoming Payment', 'sgtxt_MR8M', 'sgtxt_Revaluation from a price change', 'sgtxt_Test Payment', 'sgtxt_adding stock for Backorder test', 'vbund_C002', 'vorgn_RFIG', 'vorgn_RMPR', 'vorgn_RMRP', 'vorgn_RMWA', 'vorgn_RMWE', 'vorgn_RMWL', 'vorgn_SD00', 'kokrs_CYMB', 'kostl_AP016000', 'kostl_CA017000', 'kostl_EU014000', 'kostl_EU016000', 'kostl_US011150', 'kostl_US011770', 'kostl_US014000', 'kostl_US015000', 'kostl_US016000', 'kostl_US017000', 'kostl_US024000', 'mwsk1_O1', 'landl_CA', 'landl_CN', 'landl_DE', 'landl_IE', 'landl_NL', 'meins_CS', 'meins_EA', 'meins_G', 'meins_KAR', 'meins_KG', 'meins_L', 'meins_LB', 'erfme_CS', 'erfme_EA', 'erfme_G', 'erfme_KAR', 'erfme_KG', 'erfme_L', 'erfme_LB', 'bprme_G', 'bprme_KG', 'bprme_L', 'bprme_LB', 'vprsv_V', 'bustw_RE02', 'bustw_RE14', 'bustw_RE21', 'bustw_WA01', 'bustw_WA04', 'bustw_WE01', 'txjcd_CA0000000', 'txjcd_CANS', 'txjcd_CAON', 'txjcd_GA0000000', 'hwmet_E', 'kkber_EU01', 'kkber_USA1', 'squan_-', 'squan_0', 'operation_flag_y_L', 'operation_flag_y_U']\n",
      "Parsing date/time for column: wwert\n",
      "Parsing date/time for column: augcp\n",
      "Parsing date/time for column: valut\n",
      "Parsing date/time for column: zfbdt\n",
      "Parsing date/time for column: recordstamp_x\n",
      "Parsing date/time for column: recordstamp_y\n",
      "Remaining object columns after parsing date/time: ['usnam', 'xblnr', 'awkey', 'zuonr', 'kunnr', 'lifnr', 'matnr', 'prctr', 'pprct']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ==========================\n",
    "# 1) Load the Dataset\n",
    "# ==========================\n",
    "file_path = \"/Users/User2/Desktop/Research Seminar/timegan_fully_scaled_data.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "print(\"Original Data Shape:\", df.shape)\n",
    "\n",
    "# --------------------------\n",
    "# Quick check: which columns are still object/string?\n",
    "# --------------------------\n",
    "obj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"String/object columns:\", obj_cols)\n",
    "\n",
    "# ==========================\n",
    "# 2) Convert Boolean Columns to 0/1 (if any)\n",
    "# ==========================\n",
    "bool_cols = df.select_dtypes(include=[\"bool\"]).columns.tolist()  # or manually identify if needed\n",
    "if bool_cols:\n",
    "    print(\"Converting boolean columns to int:\", bool_cols)\n",
    "    df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "# Re-check object columns after bool conversion\n",
    "obj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# ==========================\n",
    "# 3) Parse Date/Time-Like Columns\n",
    "# ==========================\n",
    "# Some columns clearly have date strings (e.g., '2018-01-25'), others might have date+time stamps.\n",
    "# We'll parse them as datetime and convert to UNIX seconds. \n",
    "# Adjust the list below as needed.\n",
    "\n",
    "date_like_cols = [\n",
    "    \"wwert\",         # had values like '2018-01-25'\n",
    "    \"augcp\",         # date-like e.g. '2022-03-28'\n",
    "    \"valut\",         # date-like\n",
    "    \"zfbdt\",         # date-like\n",
    "    \"recordstamp_x\", # has timestamps like '2022-01-31 20:41:45.693184+00:00'\n",
    "    \"recordstamp_y\"  # similar\n",
    "    # etc. Add any other columns that are obviously date/time\n",
    "]\n",
    "\n",
    "for col in date_like_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"Parsing date/time for column: {col}\")\n",
    "        # Parse date/time\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "        # Optionally fill NaT with a default date if needed\n",
    "        df[col] = df[col].fillna(pd.to_datetime(\"1970-01-01\"))\n",
    "        # Convert to integer seconds (UNIX epoch)\n",
    "        df[col] = df[col].astype(np.int64) // 10**9\n",
    "\n",
    "# Re-check object columns after date/time conversion\n",
    "obj_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "print(\"Remaining object columns after parsing date/time:\", obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b0333d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency encoding column: usnam\n",
      "Frequency encoding column: xblnr\n",
      "Frequency encoding column: awkey\n",
      "Frequency encoding column: zuonr\n",
      "Frequency encoding column: kunnr\n",
      "Frequency encoding column: lifnr\n",
      "Frequency encoding column: matnr\n",
      "Frequency encoding column: prctr\n",
      "Frequency encoding column: pprct\n",
      "✅ All columns are now numeric.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 334626 entries, 0 to 334625\n",
      "Columns: 285 entries, mandt to operation_flag_y_U\n",
      "dtypes: float64(90), int64(195)\n",
      "memory usage: 727.6 MB\n",
      "None\n",
      "Sample of DataFrame:\n",
      "    mandt     belnr  gjahr     monat     cputm       wwert  usnam   xblnr  \\\n",
      "0    1.0  0.198683    0.0  0.000000  0.420839  1516838400    890  254627   \n",
      "1    1.0  0.198683    0.0  0.000000  0.420839  1516838400    890  254627   \n",
      "2    1.0  0.538970    0.0  0.272727  0.236498  1522800000   4566  254627   \n",
      "\n",
      "      stblg  stjah  ...  txjcd_CANS  txjcd_CAON  txjcd_GA0000000  hwmet_E  \\\n",
      "0  0.006623    0.0  ...           0           0                0        1   \n",
      "1  0.006623    0.0  ...           0           0                0        1   \n",
      "2  0.006623    0.0  ...           0           0                0        1   \n",
      "\n",
      "   kkber_EU01  kkber_USA1  squan_-  squan_0  operation_flag_y_L  \\\n",
      "0           0           0        1        0                   1   \n",
      "1           0           0        1        0                   1   \n",
      "2           0           0        0        0                   0   \n",
      "\n",
      "   operation_flag_y_U  \n",
      "0                   0  \n",
      "1                   0  \n",
      "2                   0  \n",
      "\n",
      "[3 rows x 285 columns]\n",
      "Scaling the following columns: ['wwert', 'augcp', 'valut', 'zfbdt', 'recordstamp_x', 'recordstamp_y', 'usnam', 'xblnr', 'awkey', 'zuonr', 'kunnr', 'lifnr', 'matnr', 'prctr', 'pprct']\n",
      "               wwert          augcp          valut          zfbdt  \\\n",
      "count  334626.000000  334626.000000  334626.000000  334626.000000   \n",
      "mean        0.518950       0.970415       0.639446       0.357521   \n",
      "std         0.249131       0.035058       0.083168       0.136756   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.296824       0.972222       0.585586       0.317634   \n",
      "50%         0.513143       0.972222       0.594595       0.317634   \n",
      "75%         0.815444       0.972222       0.693694       0.317634   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "       recordstamp_x  recordstamp_y          usnam          xblnr  \\\n",
      "count  334626.000000  334626.000000  334626.000000  334626.000000   \n",
      "mean        0.611513       0.611511       0.765627       0.760978   \n",
      "std         0.144803       0.144598       0.362303       0.426430   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.529785       0.529791       0.306245       1.000000   \n",
      "50%         0.584871       0.584411       0.984034       1.000000   \n",
      "75%         0.628379       0.628381       1.000000       1.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "               awkey          zuonr          kunnr          lifnr  \\\n",
      "count  334626.000000  334626.000000  334626.000000  334626.000000   \n",
      "mean        0.031558       0.439935       0.898632       0.724375   \n",
      "std         0.092126       0.492085       0.300541       0.433050   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000007       1.000000       0.077208   \n",
      "50%         0.000000       0.025935       1.000000       1.000000   \n",
      "75%         0.030303       1.000000       1.000000       1.000000   \n",
      "max         1.000000       1.000000       1.000000       1.000000   \n",
      "\n",
      "               matnr          prctr          pprct  \n",
      "count  334626.000000  334626.000000  334626.000000  \n",
      "mean        0.332081       0.870895       0.996468  \n",
      "std         0.439602       0.334782       0.059326  \n",
      "min         0.000000       0.000000       0.000000  \n",
      "25%         0.039390       1.000000       1.000000  \n",
      "50%         0.046971       1.000000       1.000000  \n",
      "75%         1.000000       1.000000       1.000000  \n",
      "max         1.000000       1.000000       1.000000  \n",
      "Data shape (N, D): (334626, 285)\n",
      "Sequences shape: (334597, 30, 285)\n",
      "Train shape: (267677, 30, 285) | Test shape: (66920, 30, 285)\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# 4) Frequency Encode Remaining String Columns\n",
    "# ==========================\n",
    "# For columns like 'usnam', 'xblnr', 'awkey', 'kunnr', 'lifnr', 'matnr', 'prctr', 'pprct', 'zuonr' etc.\n",
    "# We'll do a quick frequency encoding. \n",
    "# If a column is actually numeric but stored as string, you may parse it, but let's keep it simple.\n",
    "\n",
    "for col in obj_cols:\n",
    "    print(f\"Frequency encoding column: {col}\")\n",
    "    freq_map = df[col].value_counts().to_dict()\n",
    "    df[col] = df[col].map(freq_map)\n",
    "\n",
    "# Now all object columns should be numeric\n",
    "obj_cols_after = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "if obj_cols_after:\n",
    "    print(\"Warning: some columns are still object type:\", obj_cols_after)\n",
    "else:\n",
    "    print(\"✅ All columns are now numeric.\")\n",
    "\n",
    "# ==========================\n",
    "# 5) Verify\n",
    "# ==========================\n",
    "print(df.info())\n",
    "print(\"Sample of DataFrame:\\n\", df.head(3))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# --------------------------\n",
    "# 5.5) Scale Date/Time and Frequency-Encoded Columns\n",
    "# --------------------------\n",
    "# Define the columns that you want to scale:\n",
    "date_cols_to_scale = [\"wwert\", \"augcp\", \"valut\", \"zfbdt\", \"recordstamp_x\", \"recordstamp_y\"]\n",
    "freq_cols = [\"usnam\", \"xblnr\", \"awkey\", \"zuonr\", \"kunnr\", \"lifnr\", \"matnr\", \"prctr\", \"pprct\"]\n",
    "\n",
    "# Combine the lists (and ensure they exist in df)\n",
    "cols_to_scale = [col for col in (date_cols_to_scale + freq_cols) if col in df.columns]\n",
    "\n",
    "print(\"Scaling the following columns:\", cols_to_scale)\n",
    "\n",
    "# Initialize the scaler and fit_transform on these columns\n",
    "scaler = MinMaxScaler()\n",
    "df[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "\n",
    "# Optional: Print summary statistics to verify they are now between 0 and 1\n",
    "print(df[cols_to_scale].describe())\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# 6) Reshape into (N - seq_len + 1, seq_len, D)\n",
    "# ==========================\n",
    "data = df.values  # shape (N, D)\n",
    "seq_len = 30      # or whichever\n",
    "N, D = data.shape\n",
    "print(f\"Data shape (N, D): {(N, D)}\")\n",
    "\n",
    "# Create sequences\n",
    "sequences = []\n",
    "for i in range(N - seq_len + 1):\n",
    "    seq_slice = data[i : i + seq_len, :]\n",
    "    sequences.append(seq_slice)\n",
    "\n",
    "sequences = np.array(sequences, dtype=np.float32)  # shape: (N - seq_len + 1, seq_len, D)\n",
    "print(\"Sequences shape:\", sequences.shape)\n",
    "\n",
    "# Optional train/test split\n",
    "train_data, test_data = train_test_split(sequences, test_size=0.2, random_state=42)\n",
    "print(\"Train shape:\", train_data.shape, \"| Test shape:\", test_data.shape)\n",
    "\n",
    "# ==========================\n",
    "# 7) Proceed to TimeGAN Model\n",
    "# ==========================\n",
    "# (At this point, everything is numeric, so you can feed 'train_data' into the TimeGAN pipeline.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c552e175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns remaining: []\n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "print(\"Non-numeric columns remaining:\", non_numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75b8b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to /Users/User2/Desktop/Research Seminar/timegan_processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================\n",
    "# Export Processed DataFrame\n",
    "# ==========================\n",
    "output_path = \"/Users/User2/Desktop/Research Seminar/timegan_processed_data.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Processed data saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eb1cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a181828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57844d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
